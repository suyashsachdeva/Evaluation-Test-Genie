{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG = 125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "\n",
    "import torch_geometric as thg \n",
    "import torch_geometric.nn as gnn  \n",
    "from torch_geometric.data import Data \n",
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipyvolume as ipv\n",
    "from sklearn.cluster import DBSCAN\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = r\"/Users/suyashsachdeva/Desktop/gsoc_data.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(PATH, 'r') as f:\n",
    "    images = f['X_jets'][:]\n",
    "    m0s = f[\"m0\"][:]\n",
    "    pts = f[\"pt\"][:]\n",
    "    ydata   = f[\"y\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_points = []\n",
    "for image, m0, pt in  zip(images, m0s, pts):\n",
    "    points_graph = []\n",
    "    for c, color in enumerate([\"red\", \"green\", \"blue\"]):\n",
    "        tracks_img = image[:, :, c] \n",
    "\n",
    "        y_coords, x_coords = np.where(tracks_img > 0)  \n",
    "        try:\n",
    "            magnitudes = tracks_img[y_coords, x_coords] \n",
    "            z_coords = np.array((m0 + pt) * (magnitudes / np.max(magnitudes)), dtype=\"int8\")*4 \n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        if color==\"red\":\n",
    "            for c in range(len(x_coords)):\n",
    "                points_graph.append([x_coords[c], y_coords[c], z_coords[c], 1, color])\n",
    "        # Form the 3D points array\n",
    "        else:\n",
    "            points = np.vstack((x_coords, y_coords, z_coords)).T\n",
    "        \n",
    "            clustering = DBSCAN(eps=1.0, min_samples=4).fit(points)\n",
    "            labels = clustering.labels_\n",
    "\n",
    "            # Filter out noise (-1 label)\n",
    "            points_filtered = points[labels != -1]\n",
    "            labels_filtered = labels[labels != -1]\n",
    "\n",
    "            # Calculate centroid and radius for each cluster\n",
    "            unique_labels = set(labels_filtered)\n",
    "            centroids = np.array([points_filtered[labels_filtered == label].mean(axis=0) for label in unique_labels])\n",
    "            radii = np.array([np.sqrt((points_filtered[labels_filtered == label].shape[0]) / np.pi) for label in unique_labels])\n",
    "\n",
    "            # Normalize radii for visualization purposes\n",
    "            try:\n",
    "                radii_normalized = radii / np.max(radii) * 2  # Scale radii for better visualization\n",
    "                for centroid, radius in zip(centroids, radii_normalized):\n",
    "                    points_graph.append([x_coords[c], y_coords[c], z_coords[c], radius, color])\n",
    "            except:\n",
    "                pass\n",
    "    dataset_points.append(points_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = []\n",
    "for points_graph in dataset_points:\n",
    "    G = nx.Graph()\n",
    "    for i, point in enumerate(points_graph):\n",
    "        features = np.zeros(4, dtype=\"float32\")\n",
    "        if point[-1]==\"red\":\n",
    "            features[0] = 1.0\n",
    "        elif point[-1]==\"blue\":\n",
    "            features[1] = 1.0 \n",
    "        else: \n",
    "            features[2] = 1.0\n",
    "        features[-1] = point[3]-1.0\n",
    "        G.add_node(i, x=features)\n",
    "\n",
    "    # Define a threshold distance for connecting nodes\n",
    "    threshold_distance = 60.0\n",
    "\n",
    "    # Add edges based on distance\n",
    "    for i in range(len(points_graph)):\n",
    "        for j in range(i+1, len(points_graph)):\n",
    "            dist = np.linalg.norm(np.array(points_graph[i][:3]) - np.array(points_graph[j][:3]))\n",
    "            if dist <= threshold_distance:\n",
    "                # The connection strength could be inversely proportional to the distance\n",
    "                strength = 1 / dist if dist != 0 else 1\n",
    "                G.add_edge(i, j, weight=strength)\n",
    "    graph_data.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.nodes[0][\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-b917f280ec3e>:3: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403216189/work/torch/csrc/utils/tensor_new.cpp:278.)\n",
      "  node_features = torch.tensor([G.nodes[node][\"x\"] for node in G.nodes()], dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "dataset = []\n",
    "for G, Y in zip(graph_data, ydata):    \n",
    "    node_features = torch.tensor([G.nodes[node][\"x\"] for node in G.nodes()], dtype=torch.float)\n",
    "    edge_list = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()\n",
    "    edge_weights = torch.tensor([G[u][v]['weight'] for u, v in G.edges()], dtype=torch.float)\n",
    "    y = torch.tensor(Y)\n",
    "    data = Data(x=node_features, edge_index=edge_list, edge_attr=edge_weights, y=y)\n",
    "    dataset.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [data for data in DataLoader(dataset, batch_size=1, shuffle=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
    "\n",
    "class GCNBlock(nn.Module):\n",
    "    def __init__(self, indim, outdim, num_clusters):\n",
    "        super(GCNBlock, self).__init__()\n",
    "        self.gcn1 = gnn.GraphSAGE(indim, outdim,num_layers=2)\n",
    "        self.norm1 = gnn.BatchNorm(outdim, momentum=0.9)\n",
    "        self.gcn2 = gnn.GraphSAGE(outdim, outdim, num_layers=2)\n",
    "        self.norm2 = gnn.BatchNorm(outdim, momentum=0.9)\n",
    "        self.gcn3 = gnn.GraphSAGE(outdim, outdim, num_layers=2)\n",
    "        self.norm3 = gnn.BatchNorm(outdim, momentum=0.9)\n",
    "        self.assignment_matrix_layer = nn.Linear(outdim, num_clusters)\n",
    "\n",
    "    def forward(self, h, g, g_w, mask=None):\n",
    "        h0 = torch.relu(self.norm1(self.gcn1(h, g, g_w)))\n",
    "        h = torch.relu(self.norm2(self.gcn2(h0, g, g_w)))\n",
    "        h = torch.relu(self.norm3(self.gcn3(h, g, g_w))) + h0\n",
    "\n",
    "        adj = self.adj_from_edge(g, g_w)\n",
    "        \n",
    "        s = self.assignment_matrix_layer(h)\n",
    "        h_pool, adj_pool, _, _ = gnn.dense_mincut_pool(h, adj, s, mask=mask)\n",
    "        adj_pool = adj_pool.squeeze()\n",
    "        s_softmax = F.softmax(s, dim=-1)\n",
    "        h_pool = torch.matmul(s_softmax.t(), h)\n",
    "        g_pool, g_w_pool = self.edges_from_adj(adj_pool)\n",
    "\n",
    "        g = torch.tensor(g, dtype=torch.long)\n",
    "        return h_pool,g_pool, g_w_pool \n",
    "\n",
    "    def edges_from_adj(self, adj):\n",
    "        edges = []\n",
    "        edge_weights = []\n",
    "\n",
    "        for i in range(adj.size(0)):\n",
    "            for j in range(adj.size(1)):\n",
    "                weight = adj[i, j]\n",
    "                if weight != 0:\n",
    "                    edges.append([i, j])\n",
    "                    edge_weights.append(weight)\n",
    "\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t()\n",
    "        edge_weight = torch.tensor(edge_weights, dtype=torch.float)\n",
    "\n",
    "        return edge_index, edge_weight\n",
    "\n",
    "    def adj_from_edge(self, edge_index, edge_weight):\n",
    "        num_nodes = torch.max(edge_index).item() + 1\n",
    "        adj = torch.zeros((num_nodes, num_nodes), dtype=torch.float32)\n",
    "\n",
    "        for i, (src, dest) in enumerate(edge_index.t()):\n",
    "            adj[src, dest] = edge_weight[i]\n",
    "\n",
    "        return adj\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim=4, hidden_dim=16, clusters=128):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.conv1 = GCNBlock(in_dim, hidden_dim, clusters)\n",
    "        self.conv2 = GCNBlock(hidden_dim, hidden_dim*2, clusters//2)\n",
    "        self.conv3 = GCNBlock(hidden_dim*2, hidden_dim*4, clusters//4)\n",
    "        self.conv4 = GCNBlock(hidden_dim*4, hidden_dim*8, clusters//8)\n",
    "        self.conv5 = GCNBlock(hidden_dim*8, hidden_dim*16, clusters//16)\n",
    "        self.dense = nn.Linear(hidden_dim*16, hidden_dim)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.classify = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, g, h, g_w, batch):\n",
    "        h, g, g_w = self.conv1(h, g, g_w)\n",
    "        h, g, g_w = self.conv2(h, g, g_w)\n",
    "        h, g, g_w = self.conv3(h, g, g_w)\n",
    "        h, g, g_w = self.conv4(h, g, g_w)\n",
    "        h, g, g_w = self.conv5(h, g, g_w)\n",
    "        h = self.drop(gnn.global_mean_pool(h, batch=batch))\n",
    "        h = torch.relu(self.dense(h))\n",
    "        return torch.sigmoid(self.classify(h))\n",
    "    \n",
    "model = Classifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_true, y_prob):\n",
    "    assert y_true.size() == y_prob.size()\n",
    "    y_prob = y_prob > 0.5\n",
    "    return (y_true == y_prob).sum().item() / y_true.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 || Learning_rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-dba70fee09f5>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  g = torch.tensor(g, dtype=torch.long)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [76] to be smaller than self [1] apart from dimension 0 and to be smaller size than src [8]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m batch \u001b[38;5;241m=\u001b[39m xtr\u001b[38;5;241m.\u001b[39mbatch\n\u001b[1;32m     22\u001b[0m ytr \u001b[38;5;241m=\u001b[39m xtr\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m )\n\u001b[0;32m---> 24\u001b[0m ypred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterian(ypred\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), ytr\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step\u001b[38;5;241m<\u001b[39mtrainum:\n",
      "File \u001b[0;32m~/anaconda3/envs/gandsoc/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gandsoc/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 81\u001b[0m, in \u001b[0;36mClassifier.forward\u001b[0;34m(self, g, h, g_w, batch)\u001b[0m\n\u001b[1;32m     79\u001b[0m h, g, g_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4(h, g, g_w)\n\u001b[1;32m     80\u001b[0m h, g, g_w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv5(h, g, g_w)\n\u001b[0;32m---> 81\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(\u001b[43mgnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_mean_pool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(h))\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassify(h))\n",
      "File \u001b[0;32m~/anaconda3/envs/gandsoc/lib/python3.9/site-packages/torch_geometric/nn/pool/glob.py:63\u001b[0m, in \u001b[0;36mglobal_mean_pool\u001b[0;34m(x, batch, size)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39mdim, keepdim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gandsoc/lib/python3.9/site-packages/torch_geometric/utils/_scatter.py:79\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduce \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     78\u001b[0m     count \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39mnew_zeros(dim_size)\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mcount\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_add_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_ones\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     count \u001b[38;5;241m=\u001b[39m count\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     82\u001b[0m     index \u001b[38;5;241m=\u001b[39m broadcast(index, src, dim)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected index [76] to be smaller than self [1] apart from dimension 0 and to be smaller size than src [8]"
     ]
    }
   ],
   "source": [
    "SPLIT = 0.7\n",
    "epochs = 100\n",
    "criterian = nn.BCELoss()\n",
    "learning_rate = 1e-3\n",
    "decay = 1e-2\n",
    "for epoch in range(epochs):\n",
    "    learning_rate = learning_rate/(epoch*decay+1)\n",
    "    print(f\"Epoch: {epoch+1}/{epochs} || Learning_rate: {learning_rate}\")\n",
    "    \n",
    "    lss = 0\n",
    "    acc = 0\n",
    "    vls = 0\n",
    "    vac = 0\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    trainum = len(dataset)*SPLIT\n",
    "    for step, xtr in enumerate(dataset):\n",
    "            # try:\n",
    "                h = xtr.x\n",
    "                g = xtr.edge_index\n",
    "                g_w = xtr.edge_attr\n",
    "                batch = xtr.batch\n",
    "                ytr = xtr.y.reshape(-1,1 )\n",
    "\n",
    "                ypred = model(g,h, g_w, batch)\n",
    "                \n",
    "                loss = criterian(ypred.reshape(-1, 1), ytr.reshape(-1,1))\n",
    "                if step<trainum:\n",
    "                    acc = acc+ get_accuracy(ypred, ytr)\n",
    "                    lss = lss+loss\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                        vac = vac + get_accuracy(ypred, ytr)\n",
    "                        vls = vls+loss                         \n",
    "        \n",
    "    print(f\"Dataset:  Loss: {lss/(trainum+1):.4f} || Accuracy: {acc/(trainum+1):.4f} || Validation Loss: {vls/(step-trainum+1):.4f} || Validation Accuracy: {vac/(step-trainum+1):.4f}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gandsoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
